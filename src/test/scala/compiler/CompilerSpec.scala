package compiler

import org.scalatest.FunSuite
/** CompilerSpec.
 *
 *  Test suites for compiler functions.
 */
class CompilerSpec extends FunSuite {
  /** Test suite for program tokenization.
   *
   *  Each sequence of tokens should contain only 8 valid BrainFuck's tokens: ">", "<", "+", "-", ".", ",", "[" and "]".
   *  Any other characters should be dropped during tokenization.
   */
  test("Tokenization - Case 1:\n\">+<\" should be tokenized into: '>', '+' and '<'.") {
    assert(
      Compiler.tokenize(">+<")
      ==
      Seq(
        '>', '+', '<'
      )
    )
  }
  test("Tokenization - Case 2:\n\">,[>,]<[<]>[.>] # This acts like UNIX cat command\" should be tokenized into: '>', ',', '[', '>', ',', ']', '<', '[', '<', ']', '>', '[', '.', '>' and ']'.") {
    assert(
      Compiler.tokenize(">,[>,]<[<]>[.>] # This acts like UNIX cat command")
      ==
      Seq(
        '>', ',',
        '[',
          '>', ',',
        ']',
        '<',
        '[',
          '<',
        ']',
        '>',
        '[',
          '.', '>',
        ']'
      )
    )
  }
  test("Tokenization - Case 3:\n\"++\n> +++++\n\n[\n\t< +\n\t> -\n]\" should be tokenized into: '+', '+', '>', '+', '+', '+', '+', '+', '[', '<', '+', '>', '-' and ']'.") {
    assert(
      Compiler.tokenize("++\n> +++++\n\n[\n\t< +\n\t> -\n]")
      ==
      Seq(
        '+', '+',
        '>', '+', '+', '+', '+', '+',
        '[',
          '<', '+',
          '>', '-',
        ']'
      )
    )
  }
  test("Tokenization - Case 4:\n\"+++++++\n++++ ++++\n[\n\t< +++ +++\n\t> -\n]\n< .\" should be tokenized into: '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '[', '<', '+', '+', '+', '+', '+', '+', '>', '-', ']', '<' and '.'.") {
    assert(
      Compiler.tokenize("+++++++\n++++ ++++\n[\n\t< +++ +++\n\t> -\n]\n< .")
      ==
      Seq(
        '+', '+', '+', '+', '+', '+', '+',
        '+', '+', '+', '+', '+', '+', '+', '+',
        '[',
          '<', '+', '+', '+', '+', '+', '+',
          '>', '-',
        ']',
        '<', '.'
      )
    )
  }
  test("Tokenization - Case 5:\n\"+++++ +++++\n[\n\t> +++++ ++\n\t> +++++ +++++\n\t> +++\n\t> +\n\t<<<< -\n]\n> ++ .\n> + .\n+++++ ++ .\n.\n+++ .\n> ++ .\n<< +++++ +++++ +++++ .\n> .\n+++ .\n----- - .\n----- --- .\n> + .\n> .\" should be tokenized into: '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '[', '>', '+', '+', '+', '+', '+', '+', '+', '>', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '>', '+', '+', '+', '>', '+', '<', '<', '<', '<' '-', ']', '>', '+', '+', '.', '>', '+', '.', '+', '+', '+', '+', '+', '+', '+', '.', '.', '+', '+', '+', '.', '>', '+', '+', '.', '<', '<', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+' '.', '>', '.', '+', '+', '+', '.', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '.', '>', '+', '.', '>' and '.'.") {
    assert(
      Compiler.tokenize("+++++ +++++\n[\n\t> +++++ ++\n\t> +++++ +++++\n\t> +++\n\t> +\n\t<<<< -\n]\n> ++ .\n> + .\n+++++ ++ .\n.\n+++ .\n> ++ .\n<< +++++ +++++ +++++ .\n> .\n+++ .\n----- - .\n----- --- .\n> + .\n> .")
      ==
      Seq(
        '+', '+', '+', '+', '+', '+', '+', '+', '+', '+',
        '[',
          '>', '+', '+', '+', '+', '+', '+', '+',
          '>', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+',
          '>', '+', '+', '+',
          '>', '+',
          '<', '<', '<', '<', '-',
        ']',
        '>', '+', '+', '.',
        '>', '+', '.',
        '+', '+', '+', '+', '+', '+', '+', '.',
        '.',
        '+', '+', '+', '.',
        '>', '+', '+', '.',
        '<', '<', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '.',
        '>', '.',
        '+', '+', '+', '.',
        '-', '-', '-', '-', '-', '-', '.',
        '-', '-', '-', '-', '-', '-', '-', '-', '.',
        '>', '+', '.',
        '>', '.'
      )
    )
  }
}
