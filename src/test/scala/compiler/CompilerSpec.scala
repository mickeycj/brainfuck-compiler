package compiler

import org.scalatest.FunSuite
/** CompilerSpec.
 *
 *  Test suites for compiler functions.
 */
class CompilerSpec extends FunSuite {
  /** Test suite for program tokenization.
   *
   *  Each sequence of tokens should contain only 8 valid BrainFuck's tokens: ">", "<", "+", "-", ".", ",", "[" and "]".
   *  Any other characters should be dropped during tokenization.
   */
  test("\">+<\" should be tokenized into: \">\", \"+\" and \"<\".") {
    assert(
      Compiler.tokenize(">+<")
      ==
      Seq(
        ">", "+", "<"
      )
    )
  }
  test("\">,[>,]<[<]>[.>] # This acts like UNIX cat command\" should be tokenized into: \">\", \",\", \"[\", \">\", \",\", \"]\", \"<\", \"[\", \"<\", \"]\", \">\", \"[\", \".\", \">\" and \"]\".") {
    assert(
      Compiler.tokenize(">,[>,]<[<]>[.>] # This acts like UNIX cat command")
      ==
      Seq(
        ">", ",",
        "[",
          ">", ",",
        "]",
        "<",
        "[",
          "<",
        "]",
        ">",
        "[",
          ".", ">",
        "]"
      )
    )
  }
  test("\"++\n> +++++\n\n{\n\t< +\n\t> -\n]\" should be tokenized into: \"+\", \"+\", \">\", \"+\", \"+\", \"+\", \"+\", \"+\", \"[\", \"<\", \"+\", \">\", \"-\" and \"]\".") {
    assert(
      Compiler.tokenize("++\n> +++++\n\n{\n\t< +\n\t> -\n]")
      ==
      Seq(
        "+", "+",
        ">", "+", "+", "+", "+", "+",
        "[",
          "<", "+",
          ">", "-",
        "]"
      )
    )
  }
  test("\"+++++++\n++++ ++++\n[\n\t< +++ +++\n\t> -\n]\n< .\" should be tokenized into: \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"[\", \"<\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \">\", \"-\", \"]\", \"<\" and \".\".") {
    assert(
      Compiler.tokenize("+++++++\n++++ ++++\n[\n\t< +++ +++\n\t> -\n]\n< .")
      ==
      Seq(
        "+", "+", "+", "+", "+", "+", "+",
        "+", "+", "+", "+", "+", "+", "+", "+",
        "[",
          "<", "+", "+", "+", "+", "+", "+",
          ">", "-",
        "]",
        "<", "."
      )
    )
  }
  test("\"+++++ +++++\n[\n\t> +++++ ++\n\t> +++++ +++++\n\t> +++\n\t> +\n\t<<<< -\n]\n> ++ .\n> + .\n+++++ ++ .\n.\n+++ .\n> ++ .\n<< +++++ +++++ +++++ .\n> .\n+++ .\n----- - .\n----- --- .\n> + .\n> .\" should be tokenized into: \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"[\", \">\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \">\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \">\", \"+\", \"+\", \"+\", \">\", \"+\", \"<\", \"<\", \"<\", \"<\" \"-\", \"]\", \">\", \"+\", \"+\", \".\", \">\", \"+\", \".\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \".\", \".\", \"+\", \"+\", \"+\", \".\", \">\", \"+\", \"+\", \".\", \"<\", \"<\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\", \"+\" \".\", \">\", \".\", \"+\", \"+\", \"+\", \".\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \".\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \".\", \">\", \"+\", \".\", \">\" and \".\".") {
    assert(
      Compiler.tokenize("+++++++\n++++ ++++\n[\n\t< +++ +++\n\t> -\n]\n< .")
      ==
      Seq(
        "+", "+", "+", "+", "+", "+", "+", "+", "+", "+",
        "[",
          ">", "+", "+", "+", "+", "+", "+", "+",
          ">", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+",
          ">", "+", "+", "+",
          ">", "+",
        "[",
        ">", "+", "+", ".",
        ">", "+", ".",
        "+", "+", "+", "+", "+", "+", "+", ".",
        ".",
        "+", "+", "+", ".",
        ">", "+", "+", ".",
        "<", "<", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+", "+", ".",
        ">", ".",
        "+", "+", "+", ".",
        "-", "-", "-", "-", "-", "-", ".",
        "-", "-", "-", "-", "-", "-", "-", "-", ".",
        "+", "+", "+", ".",
        ">", "+", ".",
        ">", "."
      )
    )
  }
}
